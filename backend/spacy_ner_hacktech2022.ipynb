{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy_ner_hacktech2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "MkBDuZeBRMaT"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from typing import List, Set, Tuple\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install newspaper\n",
        "import newspaper"
      ],
      "metadata": {
        "id": "3dSbLbojSXOW"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model = spacy.load('en_core_web_sm', \n",
        "                       disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\"])"
      ],
      "metadata": {
        "id": "xms-U_HgTqEH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet') \n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5bYTa2wXX-6",
        "outputId": "e7fab112-e062-400f-f6fc-cce158584070"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "hUY6xg0jcXR5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# process a sentence using the model\n",
        "doc = nlp(\"war\")\n",
        "# It's that simple - all of the vectors and words are assigned after this point\n",
        "# Get the vector for 'text':\n",
        "doc.similarity(nlp('genocide'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ6GDReC8iaG",
        "outputId": "82d61d6d-b7b6-4bc5-b3ad-e1ac1388fabd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5655733557541108"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS = ['war', 'violence', 'attack', 'death', 'casualty', 'danger']\n",
        "COS_SIM_THRESHOLD = 0.6"
      ],
      "metadata": {
        "id": "ABWF6TSk9Aeu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def threatAssessmentPipeline(url: str,\n",
        "                             ner_model: spacy.lang.en.English,\n",
        "                             emb_model: spacy.lang.en.English,\n",
        "                             keyword_anchors: List[str],\n",
        "                             stopwords: Set[str], \n",
        "                             article_date: str = None,\n",
        "                             cos_sim_threshold: float = 0.6,\n",
        "                             threat_thresholds: Tuple[float] = (2.0, 5.0)):\n",
        "  \n",
        "  # Step 1: Extract article text from URL\n",
        "  article = newspaper.Article(url)\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  text = article.text\n",
        "\n",
        "  # Step 2: Perform NER on text\n",
        "  named_entities = ner_model(text).ents\n",
        "\n",
        "  # Step 3: Pull out specific named entities\n",
        "  ent_dict = {}\n",
        "  # Group entities by label\n",
        "  for ent in named_entities:\n",
        "    label = str(ent.label_)\n",
        "    entity = ent.text\n",
        "    if label not in ent_dict:\n",
        "      ent_dict[label] = []\n",
        "    ent_dict[label].append(str(entity))\n",
        "  # create entity frequency dict\n",
        "  ent_dict = {label:dict(Counter(entities)) for label,entities in ent_dict.items()}\n",
        "\n",
        "  # Get location\n",
        "  if ent_dict.get('LOC'):\n",
        "    location = max(ent_dict['LOC'], key=ent_dict['LOC'].get) # we only pull out the location that's mentioned most frequently\n",
        "  else:\n",
        "    location = 'LOCATION NOT FOUND'\n",
        "\n",
        "  def getMostLikelyEntities(ent_dict: dict, \n",
        "                            ent_label: str,\n",
        "                            threshold: int = 3):\n",
        "    if ent_dict.get(ent_label):\n",
        "      sorted_entities = sorted(ent_dict[ent_label].items(), key=lambda item: item[1], reverse=True)\n",
        "      candidate_tuples = sorted_entities[:min(threshold, len(sorted_entities))]\n",
        "      candidate_entities = [pair[0] for pair in candidate_tuples]\n",
        "      return candidate_entities\n",
        "    else:\n",
        "      return f'{ent_label} NOT FOUND' if ent_label in {'DATE', 'TIME'} else 'ACTORS NOT FOUND'\n",
        "  \n",
        "  # Get dates, times, and actors\n",
        "  candidate_dates = getMostLikelyEntities(ent_dict, 'DATE')\n",
        "  candidate_times = getMostLikelyEntities(ent_dict, 'TIME')\n",
        "  candidate_actors = getMostLikelyEntities(ent_dict, 'ORG')\n",
        "\n",
        "  # Step 4: Pull out relevant keywords\n",
        "  tokenCheck = lambda token: token.isalnum() and token.lower() not in stopwords # checks for alphanumeric + not a stopword\n",
        "  preprocessed_text = [lemmatizer.lemmatize(tok) for tok in word_tokenize(text) if tokenCheck(tok)]\n",
        "  TEXT_LENGTH = len(preprocessed_text)\n",
        "  freq_dict = Counter(preprocessed_text)\n",
        "  # Embed keyword anchors\n",
        "  keyword_embs = [emb_model(kw) for kw in keyword_anchors]\n",
        "  # Mine relevant keywords from article\n",
        "  key_terms = set()\n",
        "  key_term_count = 0\n",
        "  for word in freq_dict:\n",
        "    word_emb = emb_model(word)\n",
        "    if word_emb.vector.any(): # check if vector exists in pretrained model\n",
        "      for kw_emb in keyword_embs:\n",
        "        if word_emb.similarity(kw_emb) > cos_sim_threshold: # if a word is similar to any of the anchors, add it to the set\n",
        "          key_terms.add(word)\n",
        "          key_term_count += freq_dict[word]\n",
        "          break\n",
        "  raw_threat_score = (key_term_count/ARTICLE_LENGTH)*100\n",
        "  assert len(threat_thresholds) == 2, 'Exactly two threat thresholds must be provided'\n",
        "  low, high = threat_thresholds\n",
        "  if raw_threat_score < low:\n",
        "    warning = 'LOW THREAT'\n",
        "  elif low <= raw_threat_score < high:\n",
        "    warning = 'SOME THREAT'\n",
        "  else:\n",
        "    warning = 'HIGH THREAT'\n",
        "  \n",
        "  # Get list of most prevalent keywords\n",
        "  if len(key_terms) > 0:\n",
        "    key_term_freq = {k:freq_dict[k] for k in key_terms}\n",
        "    sorted_key_terms = sorted(key_term_freq.items(), key=lambda item: item[1], reverse=True)\n",
        "    candidate_tuples = sorted_key_terms[:min(5, len(sorted_key_terms))]\n",
        "    candidate_terms = [pair[0] for pair in candidate_tuples]\n",
        "  else:\n",
        "    candidate_terms = 'NO RELEVANT KEYWORDS FOUND'\n",
        "\n",
        "  # Step 5: Package it all into a dictionary\n",
        "  res = {'URL': url,\n",
        "         'Threat message': warning,\n",
        "         'Raw threat rating': raw_threat_score,\n",
        "         'Possible location': location,\n",
        "         'Possible dates': candidate_dates,\n",
        "         'Possible times': candidate_times,\n",
        "         'Possible actors': candidate_actors,\n",
        "         'Keywords': candidate_terms}\n",
        "  if article_date:\n",
        "    res['Article date': article_date]\n",
        "  \n",
        "  return res"
      ],
      "metadata": {
        "id": "JQ1tt0E5ZgQO"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test pipeline on single url\n",
        "\n",
        "# constants for testing\n",
        "#TEST_URL = 'https://www.theguardian.com/world/2022/mar/01/ukraine-russia-civilians-missiles-kyiv-tv-tower'\n",
        "TEST_URL = 'https://www.cnn.com/2022/03/03/europe/ukraine-kharkiv-civilian-strikes-intl-cmd/index.html'\n",
        "NER_MODEL = ner_model\n",
        "EMB_MODEL = nlp\n",
        "KEYWORD_ANCHORS = KEYWORDS\n",
        "STOPWORDS = stopwords.words('english')\n",
        "THREAT_THRESHOLDS = (2.0, 4.0)"
      ],
      "metadata": {
        "id": "QG1pFUxuejaZ"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test pipeline on a negative example (not related to Russia-Ukraine war, or any other conflict)\n",
        "NEG_URL = 'https://www.cnn.com/2022/03/02/tech/apple-march-event/index.html'\n",
        "threatAssessmentPipeline(url=NEG_URL,\n",
        "                         ner_model=NER_MODEL,\n",
        "                         emb_model=EMB_MODEL,\n",
        "                         keyword_anchors=KEYWORD_ANCHORS,\n",
        "                         stopwords=STOPWORDS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AKqOveOq_Bv",
        "outputId": "cf5dac31-7b46-428f-c1c0-a9d0b3972429"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Keywords': ['invasion'],\n",
              " 'Possible actors': ['Apple', 'UFC', 'AAPL'],\n",
              " 'Possible dates': ['July 09, 2021', 'the year', 'Wednesday'],\n",
              " 'Possible location': 'LOCATION NOT FOUND',\n",
              " 'Possible times': ['10:00 a.m. PT/1:00 p.m. ET'],\n",
              " 'Raw threat rating': 0.11363636363636363,\n",
              " 'Threat message': 'LOW THREAT',\n",
              " 'URL': 'https://www.cnn.com/2022/03/02/tech/apple-march-event/index.html'}"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from apiclient.discovery import build\n",
        "except:\n",
        "  !pip install google-api-python-client\n",
        "  from apiclient.discovery import build"
      ],
      "metadata": {
        "id": "lZYfeOe5sljQ"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = 'AIzaSyDt7I6cgh6LZ3oH_pmzKknomIzkrSyNkY8'\n",
        "SEARCH_ID = '21b36bd225e5f4360' # general search engine that searches the entire web\n",
        "QUERY = 'Russia-Ukraine' # search query"
      ],
      "metadata": {
        "id": "1tQzlAxPt38Y"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resource = build(serviceName='customsearch', \n",
        "                 version='v1',\n",
        "                 developerKey=API_KEY).cse()"
      ],
      "metadata": {
        "id": "SxocoGuSt58_"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = resource.list(q=QUERY, \n",
        "                       cx=SEARCH_ID,\n",
        "                       siteSearch='www.cnn.com',\n",
        "                       siteSearchFilter='i').execute()"
      ],
      "metadata": {
        "id": "2mrdRQHJv1yo"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['items'][8]['pagemap']['metatags'][0]['og:url']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "wM6VgUIGwQvd",
        "outputId": "060d421d-ff8d-453a-a1ad-ac23cac7492a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://www.cnn.com/travel/article/russia-ukraine-hurt-travel-recovery-cmd/index.html'"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DOMAINS_TO_QUERY = ['www.cnn.com', \n",
        "                    'www.msn.com',\n",
        "                    'www.foxnews.com',\n",
        "                    'www.nytimes.com',\n",
        "                    'www.news.google.com',\n",
        "                    'www.washingtonpost.com',\n",
        "                    'www.nypost.com',\n",
        "                    'www.cnbc.com',\n",
        "                    'www.news.yahoo.com',\n",
        "                    'www.dailymail.co.uk',\n",
        "                    'www.bbc.com',\n",
        "                    'www.usatoday.com',\n",
        "                    'www.people.com',\n",
        "                    'www.theguardian.com',\n",
        "                    'www.nbcnews.com',\n",
        "                    'www.businessinsider.com',\n",
        "                    'www.forbes.com',\n",
        "                    'www.huffpost.com',\n",
        "                    'www.usnews.com',\n",
        "                    'www.thehill.com',\n",
        "                    'www.bloomberg.com']"
      ],
      "metadata": {
        "id": "sNd0cV-fwRU_"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threat_assessments = [] # list of pipeline results\n",
        "for dom in tqdm(DOMAINS_TO_QUERY):\n",
        "  print()\n",
        "  print(f'Querying {dom} . . .')\n",
        "  search_results = resource.list(q=QUERY, \n",
        "                       cx=SEARCH_ID,\n",
        "                       siteSearch=dom,\n",
        "                       siteSearchFilter='i').execute()\n",
        "  try:\n",
        "    search_items = search_results['items']\n",
        "    print('Looping over search results, analyzing articles . . .')\n",
        "    for i in tqdm(range(len(search_items))):\n",
        "      res = search_items[i]\n",
        "      url = res['pagemap']['metatags'][0]['og:url']\n",
        "      try:\n",
        "        threat_assessment = threatAssessmentPipeline(url=url,\n",
        "                            ner_model=NER_MODEL,\n",
        "                            emb_model=EMB_MODEL,\n",
        "                            keyword_anchors=KEYWORD_ANCHORS,\n",
        "                            stopwords=STOPWORDS)\n",
        "        threat_assessments.append(threat_assessment)\n",
        "      except:\n",
        "        continue\n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "ojBfCsm0xBBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "Cp3zLqg_0MxS"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWT5nzgH3ihk",
        "outputId": "ff77859d-46b7-434e-ce93-29a26a32af12"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threat_assessments"
      ],
      "metadata": {
        "id": "sGGWABZL286y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Returned {len(threat_assessments)} threat assessments from {len(DOMAINS_TO_QUERY)} domains')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejHMPn7h2-ID",
        "outputId": "740a9e21-ae37-4adc-ed49-4eee8b308714"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Returned 148 threat assessments from 21 domains\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/hacktech2022/threat_assessments.json\", \"w\") as outfile:\n",
        "    json.dump(threat_assessments, outfile)"
      ],
      "metadata": {
        "id": "aA_Txgof3MK6"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4s3Fh_B73xTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}